# -*- coding: utf-8 -*-
"""House_price_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bq9PP9hpN32wD0swEqVakN_b53TWIK6_
"""

#@title
import pandas as pd 
Data=pd.read_csv("https://hackveda.in/sistec/Housing_Modified.csv")
Data.head(3)

Data

Data.tail(2)

Data.corr()#correlation to identify influencing variable

Data.shape
print ("%d rows and %d column" % Data.shape)

Data.columns

Data[["price","lotsize","bedrooms","airco"]] #multiple column

Data["price"].max()
print("Max price",Data["price"].max())
print("Min price",Data["price"].min())
print("mean of price",Data["price"].mean())
print("sum of all prices",Data["price"].sum())
Data["bedrooms"].unique()

#unique value from data
Data["airco"].unique()

Data["stories"].unique()

import matplotlib.pyplot as plt
plt.scatter(Data["lotsize"],Data["price"])
plt.xlabel("lotsize")
plt.ylabel("Price")
plt.title("Price versus lotsize")

Data.plot()

plt.plot(Data[["lotsize","bedrooms"]],Data["price"])

plt.scatter(Data["bedrooms"],Data["price"] ,color="green")

plt.scatter(Data["lotsize"],Data["price"],color="red")
plt.plot(Data["lotsize"],Data["price"],color="green")
plt.xlabel("Price")
plt.ylabel("lotsize")
plt.title("Compare price vs lotsize")

Data.info()

#check for missing values
Data.isna().any()

#Preprocessing
import sklearn.preprocessing as pp
lb=pp.LabelBinarizer()
Data.driveway=lb.fit_transform(Data.driveway)
Data.recroom=lb.fit_transform(Data.recroom)
Data.fullbase=lb.fit_transform(Data.fullbase)
Data.gashw=lb.fit_transform(Data.gashw)
Data.airco=lb.fit_transform(Data.airco)
Data.prefarea=lb.fit_transform(Data.prefarea)
Data.head(10)

le=pp.LabelEncoder()
Data.stories=le.fit_transform(Data.stories)
Data.head(10)

Data.info()

Data.corr()# After preprocessing correlation

print("Max value of lotsize before transformation",Data["lotsize"].max())
print("Min value of lotsize before transformation",Data["lotsize"].min())
#data transformation
# transform using standard score (standardization)
X=Data["lotsize"] #independent 
mean=X.mean()
Xstd=X.std()
Xnorm=(X-mean)/Xstd
print("X normalised after transformation")
Xnorm

independent=Data.columns
independent=independent.delete(0)
print("Independent variable",independent)

X=Data[independent]
y=Data["price"]
from sklearn.preprocessing import StandardScaler
scale=StandardScaler()
Xnorm=scale.fit_transform(X)
print(" X Normalised data using standardization")
Xnorm

#feature scaling
min=X.min()
max=X.max()
Xnorm=(X-min)/(max-min)
Xnorm
print("Min value after transformation", Xnorm.min())
print("Max value after transformation", Xnorm.max())

min=X.min()
min

max=X.max()
max

Xnorm #transformed data

Xnorm.plot()

Xnorm.to_csv('norm.csv', index=False,header=False)
pd.read_csv('norm.csv')

import matplotlib.pyplot as plt
plt.scatter(Xnorm["lotsize"],Data["price"])
plt.scatter(Xnorm["bedrooms"],Data["price"])
plt.scatter(Xnorm["bathrms"],Data["price"])

import seaborn as sns
sns.heatmap(Data.corr(),annot=True)

from sklearn.model_selection import train_test_split
X_train , X_test, Y_train, Y_test = train_test_split(Xnorm,y,train_size=0.80,random_state=1)
print("X_train",X_train.shape)
print("X_test",X_test.shape)
print("Y_train",Y_train.shape)
print("Y_test",Y_test.shape)

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(X_train,Y_train)
lr.score(X_test,Y_test)

lr.score(X_train,Y_train)

pred=lr.predict(X_test)
pred
plt.scatter(Y_test,pred)

from sklearn.linear_model import LogisticRegression
ls=LogisticRegression(C=1.0,solver='liblinear',random_state=0)
ls.fit(X_train,Y_train)
ls.score(X_test,Y_test)

ls.score(X_train,Y_train)

pred1=ls.predict(X_test)
pred1
plt.scatter(Y_test,pred1)

from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(n_estimators=100,max_depth=10,random_state=101)
regressor.fit(X_train,Y_train)
regressor.score(X_train,Y_train)

regressor.score(X_test,Y_test)

pred2=regressor.predict(X_test)
pred2

plt.scatter(pred2 ,X_test["lotsize"],color="green")
plt.scatter(Y_test,X_test["lotsize"],color="red")
plt.xlabel("Price")
plt.ylabel("Lotsize")
plt.title("Machine Learning with actual vs predicted")

from sklearn.tree import DecisionTreeRegressor
DTR = DecisionTreeRegressor(random_state=1)
DTR.fit(X_train,Y_train)
DTR.score(X_test,Y_test)

DTR.score(X_train,Y_train)

from sklearn.linear_model import Ridge
rr = Ridge(alpha=0.01)
rr.fit(X_train,Y_train)
rr.score(X_test,Y_test)

rr.score(X_train,Y_train)

from xgboost import XGBRegressor
reg = XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5)
reg.fit(X_train, Y_train)
reg.score(X_train, Y_train)

reg.score(X_test,Y_test)

from sklearn.ensemble import GradientBoostingRegressor
gradient_boosting_regressor= GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,
                                   max_depth=4, max_features='sqrt',
                                   min_samples_leaf=15, min_samples_split=10, 
                                   loss='huber')
gradient_boosting_regressor.fit(X_train, Y_train)
gradient_boosting_regressor.score(X_train, Y_train)

gradient_boosting_regressor.score(X_test, Y_test)

import lightgbm as lgb
lgbm_regressor= lgb.LGBMRegressor(objective='regression',num_leaves=5,
                              learning_rate=0.05, n_estimators=720,
                              max_bin = 55, bagging_fraction = 0.8,
                              bagging_freq = 5, feature_fraction = 0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)
lgbm_regressor.fit(X_train,Y_train)
lgbm_regressor.score(X_train, Y_train)

lgbm_regressor.score(X_test, Y_test)

from sklearn.linear_model import Lasso
Llr = Lasso(alpha=0.01)
Llr.fit(X_train,Y_train)
Llr.score(X_train,Y_train)

Llr.score(X_test,Y_test)

#FEATURE SELECTION
#RFE (Recursive Feature Elimination)
from sklearn.feature_selection import RFE
import numpy as np
model = LinearRegression()
#Initializing RFE model
rfe = RFE(model, 10)
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)
print(rfe.support_)
print(rfe.ranking_)
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model = LinearRegression()
    rfe = RFE(model,nof_list[n])
    X_train_rfe = rfe.fit_transform(X_train,Y_train)
    X_test_rfe = rfe.transform(X_train)
    model.fit(X_train_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

cols = list(X_train.columns)
model = LinearRegression()
#Initializing RFE model
rfe = RFE(model, 10)             
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)              
temp = pd.Series(rfe.support_,index = cols)
selected_features_rfe = temp[temp==True].index
print(selected_features_rfe)

model.score(X_test[selected_features_rfe],Y_test)

#RFE (Recursive Feature Elimination)
from sklearn.feature_selection import RFE
import numpy as np
model = Ridge()
#Initializing RFE model
rfe = RFE(model, 11)
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)
print(rfe.support_)
print(rfe.ranking_)

#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model = Ridge()
    rfe = RFE(model,nof_list[n])
    X_train_rfe = rfe.fit_transform(X_train,Y_train)
    X_test_rfe = rfe.transform(X_train)
    model.fit(X_train_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

cols = list(X_train.columns)
model = Ridge()
#Initializing RFE model
rfe = RFE(model, 11)             
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)              
temp = pd.Series(rfe.support_,index = cols)
selected_features_rfe = temp[temp==True].index
print(selected_features_rfe)

model.score(X_test[selected_features_rfe],Y_test)

#RFE (Recursive Feature Elimination)
from sklearn.feature_selection import RFE
import numpy as np
model = RandomForestRegressor()
#Initializing RFE model
rfe = RFE(model, 11)
X_rfe = rfe.fit_transform(X_train,Y_train)  
model.fit(X_rfe,Y_train)
print(rfe.support_)
print(rfe.ranking_)
nof_list=np.arange(1,11)            
high_score=0
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model = RandomForestRegressor()
    rfe = RFE(model,nof_list[n])
    X_train_rfe = rfe.fit_transform(X_train,Y_train)
    X_test_rfe = rfe.transform(X_train)
    model.fit(X_train_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

cols = list(X_train.columns)
model = RandomForestRegressor()
#Initializing RFE model
rfe = RFE(model, 9)             
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)              
temp = pd.Series(rfe.support_,index = cols)
selected_features_rfe = temp[temp==True].index
print(selected_features_rfe)

model.score(X_test[selected_features_rfe],Y_test)

#RFE (Recursive Feature Elimination)
from sklearn.feature_selection import RFE
import numpy as np
model = LogisticRegression()
#Initializing RFE model
rfe = RFE(model, 11)
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)
print(rfe.support_)
print(rfe.ranking_)
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model =LogisticRegression()
    rfe = RFE(model,nof_list[n])
    X_train_rfe = rfe.fit_transform(X_train,Y_train)
    X_test_rfe = rfe.transform(X_train)
    model.fit(X_train_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

cols = list(X_train.columns)
model = LogisticRegression()
#Initializing RFE model
rfe = RFE(model, 9)             
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)              
temp = pd.Series(rfe.support_,index = cols)
selected_features_rfe = temp[temp==True].index
print(selected_features_rfe)

model.score(X_test[selected_features_rfe],Y_test)

#RFE (Recursive Feature Elimination)
from sklearn.feature_selection import RFE
import numpy as np
model = DecisionTreeRegressor()
#Initializing RFE model
rfe = RFE(model, 11)
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)
print(rfe.support_)
print(rfe.ranking_)
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model =DecisionTreeRegressor()
    rfe = RFE(model,nof_list[n])
    X_train_rfe = rfe.fit_transform(X_train,Y_train)
    X_test_rfe = rfe.transform(X_train)
    model.fit(X_train_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

cols = list(X_train.columns)
model = DecisionTreeRegressor()
#Initializing RFE model
rfe = RFE(model, 9)             
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)              
temp = pd.Series(rfe.support_,index = cols)
selected_features_rfe = temp[temp==True].index
print(selected_features_rfe)

model.score(X_test[selected_features_rfe],Y_test)

#RFE (Recursive Feature Elimination)
from sklearn.feature_selection import RFE
import numpy as np
model = Lasso()
#Initializing RFE model
rfe = RFE(model, 11)
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)
print(rfe.support_)
print(rfe.ranking_)
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model =Lasso()
    rfe = RFE(model,nof_list[n])
    X_train_rfe = rfe.fit_transform(X_train,Y_train)
    X_test_rfe = rfe.transform(X_train)
    model.fit(X_train_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

cols = list(X_train.columns)
model = Lasso()
#Initializing RFE model
rfe = RFE(model, 10)             
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)              
temp = pd.Series(rfe.support_,index = cols)
selected_features_rfe = temp[temp==True].index
print(selected_features_rfe)

model.score(X_test[selected_features_rfe],Y_test)

#RFE (Recursive Feature Elimination)
from sklearn.feature_selection import RFE
import numpy as np
model = lgb.LGBMRegressor()
#Initializing RFE model
rfe = RFE(model, 11)
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)
print(rfe.support_)
print(rfe.ranking_)
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model =lgb.LGBMRegressor()
    rfe = RFE(model,nof_list[n])
    X_train_rfe = rfe.fit_transform(X_train,Y_train)
    X_test_rfe = rfe.transform(X_train)
    model.fit(X_train_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

cols = list(X_train.columns)
model = lgb.LGBMRegressor()
#Initializing RFE model
rfe = RFE(model, 10)             
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)              
temp = pd.Series(rfe.support_,index = cols)
selected_features_rfe = temp[temp==True].index
print(selected_features_rfe)

model.score(X_test[selected_features_rfe],Y_test)

#RFE (Recursive Feature Elimination)
from sklearn.feature_selection import RFE
import numpy as np
model = XGBRegressor()
#Initializing RFE model
rfe = RFE(model, 10)
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)
print(rfe.support_)
print(rfe.ranking_)
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model =XGBRegressor()
    rfe = RFE(model,nof_list[n])
    X_train_rfe = rfe.fit_transform(X_train,Y_train)
    X_test_rfe = rfe.transform(X_train)
    model.fit(X_train_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

cols = list(X_train.columns)
model = XGBRegressor()
#Initializing RFE model
rfe = RFE(model, 10)             
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)              
temp = pd.Series(rfe.support_,index = cols)
selected_features_rfe = temp[temp==True].index
print(selected_features_rfe)

model.score(X_test[selected_features_rfe].values,Y_test.values)

#RFE (Recursive Feature Elimination)
from sklearn.feature_selection import RFE
import numpy as np
model =GradientBoostingRegressor()
#Initializing RFE model
rfe = RFE(model, 11)
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)
print(rfe.support_)
print(rfe.ranking_)
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model =GradientBoostingRegressor()
    rfe = RFE(model,nof_list[n])
    X_train_rfe = rfe.fit_transform(X_train,Y_train)
    X_test_rfe = rfe.transform(X_train)
    model.fit(X_train_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

cols = list(X_train.columns)
model = GradientBoostingRegressor()
#Initializing RFE model
rfe = RFE(model, 10)             
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_train,Y_train)  
#Fitting the data to model
model.fit(X_rfe,Y_train)              
temp = pd.Series(rfe.support_,index = cols)
selected_features_rfe = temp[temp==True].index
print(selected_features_rfe)

model.score(X_test[selected_features_rfe],Y_test)

#forward feature selection
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
# find and remove correlated features
def correlation(Data, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = Xnorm.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X_train, 0.8)
print('correlated features: ', len(set(corr_features)) )

from mlxtend.feature_selection import SequentialFeatureSelector as SFS

sfs1 = SFS(XGBRegressor(objective ='reg:linear'), 
           k_features=11, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='r2',
           cv=0)

sfs1 = sfs1.fit(np.array(X_train), Y_train)

selected = X_train.columns[list(sfs1.k_feature_idx_)]
selected

sfs1 = sfs1.fit(X_test, Y_test)

#forward feature selection
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
# find and remove correlated features
def correlation(Data, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = Data.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X_train, 0.8)
print('correlated features: ', len(set(corr_features)) )

from mlxtend.feature_selection import SequentialFeatureSelector as SFS

sfs1 = SFS(LogisticRegression(), 
           k_features=10, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='r2',
           cv=0)

sfs1 = sfs1.fit(np.array(X_train), Y_train)

sfs1.k_feature_idx_
X_train.columns[list(sfs1.k_feature_idx_)]

sfs1 = sfs1.fit(X_test, Y_test)

#forward feature selection
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
# find and remove correlated features
def correlation(Data, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = Data.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X_train, 0.8)
print('correlated features: ', len(set(corr_features)) )

from mlxtend.feature_selection import SequentialFeatureSelector as SFS

sfs1 = SFS(Lasso(), 
           k_features=10, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='r2',
           cv=0)

sfs1 = sfs1.fit(np.array(X_train), Y_train)

sfs1.k_feature_idx_
X_train.columns[list(sfs1.k_feature_idx_)]

sfs1 = sfs1.fit(X_test, Y_test)

R=RandomForestRegressor()

#forward feature selection
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
def correlation(Data, threshold):
    col_corr = set() 
    corr_matrix = Data.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: 
                colname = corr_matrix.columns[i]
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X_train, 0.8)
print('correlated features: ', len(set(corr_features)) )
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
sfs1 = SFS(RandomForestRegressor(), 
           k_features=11, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='r2',
           cv=0)
sfs1 = sfs1.fit(np.array(X_train), Y_train)

sfs1.k_feature_idx_
dselected=X_train.columns[list(sfs1.k_feature_idx_)]
dselected

R.fit(X,y)
pred1 = R.predict(X)

Data["predicted_price"] = pred1

Data[["price","lotsize" ,"predicted_price"]]

plt.scatter(Data["price"],Data["lotsize"],color="green")
plt.scatter(Data["predicted_price"],Data["lotsize"],color="red")
plt.xlabel('Price')
plt.ylabel('Plot Area in sq mtrs')
plt.title("Actual Vs Predicted")

# make a simple app which can predict price using the model
user_data = {}
for variable in dselected:
  temp = input("Enter " + variable + ":")
  user_data[variable] = temp
user_input = pd.DataFrame(data=user_data,index=[0],columns=dselected)
R=RandomForestRegressor()
R.fit(X,y)
pred_price = R.predict(user_input)
print("Predicted price of the house is ", pred_price[0])

sfs1 = sfs1.fit(X_test, Y_test)

from mlxtend.feature_selection import SequentialFeatureSelector as SFS

sfs1 = SFS(LinearRegression(), 
           k_features=11, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='r2',
           cv=0)

sfs1 = sfs1.fit(np.array(X_train), Y_train)

sfs1 = sfs1.fit(X_test, Y_test)

from mlxtend.feature_selection import SequentialFeatureSelector as SFS

sfs1 = SFS(DecisionTreeRegressor(),
           k_features=5, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='r2',
           cv=5)

sfs1 = sfs1.fit(np.array(X_train), Y_train)

sfs1 = sfs1.fit(X_test, Y_test)

from mlxtend.feature_selection import SequentialFeatureSelector as SFS

sfs1 = SFS(Ridge(),
           k_features=9, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='r2',
           cv=5)

sfs1 = sfs1.fit(np.array(X_train), Y_train)

sfs1 = sfs1.fit(X_test, Y_test)

from mlxtend.feature_selection import SequentialFeatureSelector as SFS

sfs1 = SFS(lgb.LGBMRegressor(),
           k_features=10, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='r2',
           cv=5)

sfs1 = sfs1.fit(np.array(X_train), Y_train)

sfs1 = sfs1.fit(X_test, Y_test)

from mlxtend.feature_selection import SequentialFeatureSelector as SFS

sfs1 = SFS(GradientBoostingRegressor(),
           k_features=10, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='r2',
           cv=5)

sfs1 = sfs1.fit(np.array(X_train), Y_train)

sfs1 = sfs1.fit(X_test, Y_test)

# Calcualte the Fisher Score (chi2) between each feature and target
from sklearn.feature_selection import chi2 , SelectKBest
# find best scored 5 features
chi_selector = SelectKBest(chi2,k=8)
ch=chi_selector.fit(X_train, Y_train)
chi_support = chi_selector.get_support()
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model = LinearRegression()
    ch = SelectKBest(chi2, nof_list[n])
    X_train_rfe = ch.fit_transform(X_train,Y_train)
    X_test_rfe = ch.transform(X_train)
    X_test_ = ch.transform(X_test)
    model.fit(X_test_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

model.score(X_test_,Y_test)

# Calcualte the Fisher Score (chi2) between each feature and target
from sklearn.feature_selection import chi2 ,SelectKBest
# find best scored 5 features
chi_selector = SelectKBest(chi2, k=10)
ch=chi_selector.fit(X_train, Y_train)
chi_support = chi_selector.get_support()
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model = LogisticRegression()
    ch = SelectKBest(chi2, nof_list[n])
    X_train_rfe = ch.fit_transform(X_train,Y_train)
    X_test_rfe = ch.transform(X_train)
    X_test_ = ch.transform(X_test)
    model.fit(X_test_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

model.score(X_test_,Y_test)

# Calcualte the Fisher Score (chi2)
from sklearn.feature_selection import chi2
chi_selector = SelectKBest(chi2, k=11)
ch=chi_selector.fit(X_train, Y_train)
chi_support = chi_selector.get_support()
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model = RandomForestRegressor()
    ch = SelectKBest(chi2, nof_list[n])
    X_train_rfe = ch.fit_transform(X_train,Y_train)
    X_test_rfe = ch.transform(X_train)
    X_test_ = ch.transform(X_test)
    model.fit(X_test_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

model.score(X_test_,Y_test)

# Calcualte the Fisher Score (chi2) between each feature and target
from sklearn.feature_selection import chi2
# find best scored 5 features
chi_selector = SelectKBest(chi2, k=10)
ch=chi_selector.fit(X_train, Y_train)
chi_support = chi_selector.get_support()
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model = XGBRegressor()
    ch = SelectKBest(chi2, nof_list[n])
    X_train_rfe = ch.fit_transform(X_train,Y_train)
    X_test_rfe = ch.transform(X_train)
    X_test_ = ch.transform(X_test)
    model.fit(X_test_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

model.score(X_test_,Y_test)

# Calcualte the Fisher Score (chi2) between each feature and target
from sklearn.feature_selection import chi2
# find best scored 5 features
chi_selector = SelectKBest(chi2, k=9)
ch=chi_selector.fit(X_train, Y_train)
chi_support = chi_selector.get_support()
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model = GradientBoostingRegressor()
    ch = SelectKBest(chi2, nof_list[n])
    X_train_rfe = ch.fit_transform(X_train,Y_train)
    X_test_rfe = ch.transform(X_train)
    X_test_ = ch.transform(X_test)
    model.fit(X_test_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

model.score(X_test_,Y_test)

# Calcualte the Fisher Score (chi2) between each feature and target
from sklearn.feature_selection import chi2
# find best scored 5 features
chi_selector = SelectKBest(chi2, k=10)
ch=chi_selector.fit(X_train, Y_train)
chi_support = chi_selector.get_support()
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model = Lasso()
    ch = SelectKBest(chi2, nof_list[n])
    X_train_rfe = ch.fit_transform(X_train,Y_train)
    X_test_rfe = ch.transform(X_train)
    X_test_ = ch.transform(X_test)
    model.fit(X_test_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

model.score(X_test_,Y_test)

# Calcualte the Fisher Score (chi2) between each feature and target
from sklearn.feature_selection import chi2
# find best scored 5 features
chi_selector = SelectKBest(chi2, k=8)
ch=chi_selector.fit(X_train, Y_train)
chi_support = chi_selector.get_support()
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model = Ridge()
    ch = SelectKBest(chi2, nof_list[n])
    X_train_rfe = ch.fit_transform(X_train,Y_train)
    X_test_rfe = ch.transform(X_train)
    X_test_ = ch.transform(X_test)
    model.fit(X_test_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

model.score(X_test_,Y_test)

# Calcualte the Fisher Score (chi2) between each feature and target
from sklearn.feature_selection import chi2
# find best scored 5 features
chi_selector = SelectKBest(chi2, k=11)
ch=chi_selector.fit(X_train, Y_train)
chi_support = chi_selector.get_support()
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model = lgb.LGBMRegressor()
    ch = SelectKBest(chi2, nof_list[n])
    X_train_rfe = ch.fit_transform(X_train,Y_train)
    X_test_rfe = ch.transform(X_train)
    X_test_ = ch.transform(X_test)
    model.fit(X_test_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

model.score(X_test_,Y_test)

# Calcualte the Fisher Score (chi2) between each feature and target
from sklearn.feature_selection import chi2
# find best scored 5 features
chi_selector = SelectKBest(chi2, k=9)
ch=chi_selector.fit(X_train, Y_train)
chi_support = chi_selector.get_support()
#no of features
nof_list=np.arange(1,11)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model = DecisionTreeRegressor()
    ch = SelectKBest(chi2, nof_list[n])
    X_train_rfe = ch.fit_transform(X_train,Y_train)
    X_test_rfe = ch.transform(X_train)
    X_test_ = ch.transform(X_test)
    model.fit(X_test_rfe,Y_train)
    score = model.score(X_test_rfe,Y_train)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

model.score(X_test_,Y_test)

# Calculate Univariate Statistical measure between each variable and target
from sklearn.feature_selection import f_regression
univariates = f_regression(X_train.fillna(0),Y_train)
# Capture P values in a series
univariates = pd.Series(univariates[1])
univariates.index = X_train.columns
univariates.sort_values(ascending=False, inplace=True)
# Select K best Features
from sklearn.feature_selection import SelectKBest, SelectPercentile
k_best_features = SelectKBest(f_regression, k=10).fit(X_train.fillna(0), Y_train)
X_train.columns[k_best_features.get_support()]
X_test.columns[k_best_features.get_support()]

X_train = k_best_features.transform(X_train.fillna(0))
X_test = k_best_features.transform(X_test.fillna(0))
model=RandomForestRegressor()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

X_train.shape
model=LogisticRegression()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

X_train.shape
model=LinearRegression()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

X_train.shape
model=XGBRegressor()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

X_train.shape
model=GradientBoostingRegressor()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

pred1 = model.predict(X_test)
plt.scatter(Y_test , pred1)

X_train.shape
model=lgb.LGBMRegressor()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

X_train.shape
model=Lasso()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

X_train.shape
model=Ridge()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

X_train.shape
model=DecisionTreeRegressor()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

pred1 = model.predict(X_test)
plt.scatter(Y_test , pred1)

X_train = pd.DataFrame(X_train)
X_train

X_test = pd.DataFrame(X_test)
X_test

#Brute Force Method to find Correlated Feature
import numpy as np
import pandas as pd
def correlation(data, threshold=None):
    # Set of all names of correlated columns
    col_corr = set()
    corr_mat = Xnorm.corr() 
    for i in range(len(corr_mat.columns)):
        for j in range(i):
            if (abs(corr_mat.iloc[i,j]) > threshold):
                colname = corr_mat.columns[i]
                col_corr.add(colname)
    return col_corr

correlated_features = correlation(data=Xnorm, threshold=0.3)
len(set(correlated_features))
print(correlated_features)

X_train.pop(2)
X_train.pop(3)
X_train.pop(6)
X_train.pop(9)

X_test.pop(2)
X_test.pop(3)
X_test.pop(6)
X_test.pop(9)

X_train

X_test

model=LinearRegression()
model.fit(X_train,Y_train)

model.score(X_train,Y_train)

model.score(X_test,Y_test)

X_train.shape,X_test.shape
model=Ridge()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

X_train.shape,X_test.shape
model=Lasso()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

X_train.shape,X_test.shape
model=lgb.LGBMRegressor()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

X_train.shape,X_test.shape
model=GradientBoostingRegressor()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

X_train.shape,X_test.shape
model=XGBRegressor()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

X_train.shape,X_test.shape
model=LogisticRegression()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

X_train.shape,X_test.shape
model=DecisionTreeRegressor()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)

X_train.shape,X_test.shape
model=RandomForestRegressor()
model.fit(X_train,Y_train)
model.score(X_train,Y_train)

model.score(X_test,Y_test)